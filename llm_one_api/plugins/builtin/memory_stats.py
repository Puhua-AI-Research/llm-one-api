"""
å†…å­˜ç»Ÿè®¡æ’ä»¶

å°†ç»Ÿè®¡æ•°æ®ä¿å­˜åœ¨å†…å­˜ä¸­ï¼Œé€‚åˆå¼€å‘å’Œæµ‹è¯•
"""

from typing import Dict, Any, List
from collections import defaultdict
from datetime import datetime

from llm_one_api.plugins.interfaces.stats import StatsPlugin, RequestInfo, ResponseInfo
from llm_one_api.utils.logger import setup_logger

logger = setup_logger(__name__)


class MemoryStatsPlugin(StatsPlugin):
    """å†…å­˜ç»Ÿè®¡æ’ä»¶"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.max_records = config.get("max_records", 1000)  # æœ€å¤šä¿å­˜çš„è®°å½•æ•°
        self.requests: List[Dict] = []
        self.responses: List[Dict] = []
        self.stats_by_model = defaultdict(lambda: {
            "total_requests": 0,
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
            "total_duration": 0.0,
            "total_cost": 0.0,
        })
    
    async def record_request(self, request_info: RequestInfo):
        """
        è®°å½•è¯·æ±‚ä¿¡æ¯åˆ°å†…å­˜
        
        Args:
            request_info: è¯·æ±‚ä¿¡æ¯
        """
        record = {
            "request_id": request_info.request_id,
            "user_id": request_info.user_id,
            "model": request_info.model,
            "endpoint": request_info.endpoint,
            "stream": request_info.stream,
            "timestamp": request_info.timestamp.isoformat(),
        }
        
        self.requests.append(record)
        
        # é™åˆ¶è®°å½•æ•°é‡
        if len(self.requests) > self.max_records:
            self.requests.pop(0)
    
    async def record_response(self, response_info: Dict[str, Any]):
        """
        è®°å½•å“åº”ä¿¡æ¯åˆ°å†…å­˜
        
        Args:
            response_info: å“åº”ä¿¡æ¯å­—å…¸
        """
        model = response_info.get("model", "unknown")
        token_usage = response_info.get("token_usage", {})
        duration = response_info.get("duration", 0)
        metadata = response_info.get("metadata", {})  # ç›´æ¥è·å–ä¼ é€’çš„ metadata
        
        # è®°å½•è¯¦ç»†å“åº”
        self.responses.append(response_info)
        
        # é™åˆ¶è®°å½•æ•°é‡
        if len(self.responses) > self.max_records:
            self.responses.pop(0)
        
        # æ›´æ–°æ¨¡å‹ç»Ÿè®¡
        stats = self.stats_by_model[model]
        stats["total_requests"] += 1
        stats["total_tokens"] += token_usage.get("total_tokens", 0)
        stats["prompt_tokens"] = stats.get("prompt_tokens", 0) + token_usage.get("prompt_tokens", 0)
        stats["completion_tokens"] = stats.get("completion_tokens", 0) + token_usage.get("completion_tokens", 0)
        stats["total_duration"] += duration
        
        # è®¡ç®—å¹¶ç´¯è®¡æˆæœ¬
        cost = self._calculate_cost(token_usage, metadata)
        if cost:
            stats["total_cost"] += cost.get("total_cost", 0)
        
        # è®°å½•æ¨¡å‹é™åˆ¶ä¿¡æ¯ï¼ˆåªè®°å½•ä¸€æ¬¡ï¼‰
        if "model_info" not in stats and metadata:
            model_info = self._extract_model_info(metadata)
            if model_info:
                stats["model_info"] = model_info
    
    def get_stats(self) -> Dict[str, Any]:
        """
        è·å–ç»Ÿè®¡æ•°æ®
        
        Returns:
            ç»Ÿè®¡æ•°æ®å­—å…¸
        """
        return {
            "total_requests": len(self.responses),
            "by_model": dict(self.stats_by_model),
            "recent_requests": self.requests[-10:],  # æœ€è¿‘10æ¡è¯·æ±‚
            "recent_responses": self.responses[-10:],  # æœ€è¿‘10æ¡å“åº”
        }
    
    async def initialize(self):
        """åˆå§‹åŒ–æ’ä»¶"""
        logger.info(f"å†…å­˜ç»Ÿè®¡æ’ä»¶åˆå§‹åŒ–ï¼Œæœ€å¤§è®°å½•æ•°: {self.max_records}")
    
    def _calculate_cost(self, token_usage: Dict[str, int], metadata: Dict[str, Any]) -> Dict[str, Any]:
        """
        è®¡ç®—è¯·æ±‚æˆæœ¬
        
        Args:
            token_usage: token ä½¿ç”¨é‡
            metadata: æ¨¡å‹å…ƒæ•°æ®ï¼ˆåŒ…å«ä»·æ ¼ä¿¡æ¯ï¼‰
            
        Returns:
            æˆæœ¬ä¿¡æ¯å­—å…¸
        """
        if not token_usage or not metadata:
            return {}
        
        prompt_tokens = token_usage.get("prompt_tokens", 0)
        completion_tokens = token_usage.get("completion_tokens", 0)
        
        prompt_price = metadata.get("price_per_1k_prompt_tokens")
        completion_price = metadata.get("price_per_1k_completion_tokens")
        
        if prompt_price is None or completion_price is None:
            return {}
        
        prompt_cost = (prompt_tokens / 1000) * prompt_price
        completion_cost = (completion_tokens / 1000) * completion_price
        total_cost = prompt_cost + completion_cost
        
        return {
            "prompt_cost": round(prompt_cost, 6),
            "completion_cost": round(completion_cost, 6),
            "total_cost": round(total_cost, 6),
            "currency": "USD"
        }
    
    def _extract_model_info(self, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """
        æå–æ¨¡å‹é™åˆ¶ä¿¡æ¯
        
        Args:
            metadata: æ¨¡å‹å…ƒæ•°æ®
            
        Returns:
            æ¨¡å‹é™åˆ¶ä¿¡æ¯
        """
        if not metadata:
            return {}
        
        model_info = {}
        for field in ["max_tokens", "max_input_tokens", "max_output_tokens"]:
            if field in metadata:
                model_info[field] = metadata[field]
        
        return model_info
    
    async def cleanup(self):
        """æ¸…ç†æ’ä»¶"""
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        logger.info("=" * 60)
        logger.info("ğŸ“Š å†…å­˜ç»Ÿè®¡æ’ä»¶ - æœ€ç»ˆç»Ÿè®¡æ•°æ®")
        logger.info("=" * 60)
        logger.info(f"æ€»è¯·æ±‚æ•°: {len(self.responses)}")
        logger.info("")
        
        total_cost = 0.0
        
        for model, stats in self.stats_by_model.items():
            logger.info(f"æ¨¡å‹: {model}")
            logger.info(f"  â”œâ”€ æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
            logger.info(f"  â”œâ”€ è¾“å…¥ Token (prompt): {stats['prompt_tokens']:,}")
            logger.info(f"  â”œâ”€ è¾“å‡º Token (completion): {stats['completion_tokens']:,}")
            logger.info(f"  â”œâ”€ æ€» Token: {stats['total_tokens']:,}")
            logger.info(f"  â”œâ”€ æ€»è€—æ—¶: {stats['total_duration']:.2f}s")
            
            if stats['total_requests'] > 0:
                avg_tokens = stats['total_tokens'] / stats['total_requests']
                avg_duration = stats['total_duration'] / stats['total_requests']
                logger.info(f"  â”œâ”€ å¹³å‡ Token/è¯·æ±‚: {avg_tokens:.1f}")
                logger.info(f"  â”œâ”€ å¹³å‡è€—æ—¶/è¯·æ±‚: {avg_duration:.2f}s")
            
            # æ˜¾ç¤ºæˆæœ¬ä¿¡æ¯
            model_cost = stats.get('total_cost', 0)
            if model_cost > 0:
                logger.info(f"  â”œâ”€ æ€»æˆæœ¬: ${model_cost:.6f} USD")
                if stats['total_requests'] > 0:
                    avg_cost = model_cost / stats['total_requests']
                    logger.info(f"  â”œâ”€ å¹³å‡æˆæœ¬/è¯·æ±‚: ${avg_cost:.6f} USD")
                total_cost += model_cost
            
            # æ˜¾ç¤ºæ¨¡å‹é™åˆ¶ä¿¡æ¯
            model_info = stats.get('model_info')
            if model_info:
                limits = []
                if model_info.get('max_tokens'):
                    limits.append(f"æ€»={model_info['max_tokens']}")
                if model_info.get('max_input_tokens'):
                    limits.append(f"è¾“å…¥={model_info['max_input_tokens']}")
                if model_info.get('max_output_tokens'):
                    limits.append(f"è¾“å‡º={model_info['max_output_tokens']}")
                if limits:
                    logger.info(f"  â”œâ”€ Token é™åˆ¶: {', '.join(limits)}")
            
            logger.info(f"  â””â”€ å®Œæˆ")
            logger.info("")
        
        # æ˜¾ç¤ºæ€»æˆæœ¬
        if total_cost > 0:
            logger.info(f"ğŸ’° æ€»æˆæœ¬: ${total_cost:.6f} USD")
            logger.info("")
        
        logger.info("=" * 60)

