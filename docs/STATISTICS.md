# ç»Ÿè®¡åŠŸèƒ½æ–‡æ¡£

## ğŸ“Š ç»Ÿè®¡ä¸ŠæŠ¥åŠŸèƒ½

LLM One API æä¾›äº†å®Œæ•´çš„ç»Ÿè®¡ä¸ŠæŠ¥åŠŸèƒ½ï¼Œè‡ªåŠ¨è®°å½•æ¯ä¸ªè¯·æ±‚çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š

- âœ… **è¾“å…¥ Token** (prompt_tokens) - è¯·æ±‚æ¶ˆæ¯çš„ token æ•°é‡
- âœ… **è¾“å‡º Token** (completion_tokens) - ç”Ÿæˆå“åº”çš„ token æ•°é‡
- âœ… **æ€» Token** (total_tokens) - è¾“å…¥+è¾“å‡ºçš„æ€»è®¡
- âœ… **è¯·æ±‚è€—æ—¶** - å®Œæ•´è¯·æ±‚çš„å¤„ç†æ—¶é—´
- âœ… **æ¨¡å‹ä¿¡æ¯** - ä½¿ç”¨çš„æ¨¡å‹åç§°
- âœ… **ç”¨æˆ·ä¿¡æ¯** - API Key æ ‡è¯†
- âœ… **æµå¼æ ‡è¯†** - æ˜¯å¦ä¸ºæµå¼è¯·æ±‚

## ğŸ”Œ ç»Ÿè®¡æ’ä»¶

### 1. æ—¥å¿—ç»Ÿè®¡æ’ä»¶ (log)

å°†ç»Ÿè®¡ä¿¡æ¯è¾“å‡ºåˆ°æ—¥å¿—æ–‡ä»¶æˆ–æ§åˆ¶å°ã€‚

#### é…ç½®

```yaml
plugins:
  stats: ["log"]

stats:
  log:
    format: "json"  # æˆ– "text"
```

#### JSON æ ¼å¼è¾“å‡º

```json
{
  "event": "response",
  "model": "gpt-3.5-turbo",
  "user": "sk-test-123",
  "endpoint": "chat",
  "stream": false,
  "duration": 2.5,
  "timestamp": "2025-11-12T10:00:00.123456",
  "tokens": {
    "prompt_tokens": 50,
    "completion_tokens": 120,
    "total_tokens": 170
  }
}
```

**é€‚ç”¨åœºæ™¯**ï¼š
- éœ€è¦è§£ææ—¥å¿—è¿›è¡Œåˆ†æ
- é›†æˆåˆ°æ—¥å¿—æ”¶é›†ç³»ç»Ÿï¼ˆå¦‚ ELKã€Splunkï¼‰
- è‡ªåŠ¨åŒ–ç»Ÿè®¡åˆ†æ

#### æ–‡æœ¬æ ¼å¼è¾“å‡º

```
ğŸ“Š å“åº”ç»Ÿè®¡ | æ¨¡å‹=gpt-3.5-turbo | ç”¨æˆ·=sk-test-123 | è€—æ—¶=2.50s | è¾“å…¥Token=50 | è¾“å‡ºToken=120 | æ€»Token=170 | æµå¼=False
```

**é€‚ç”¨åœºæ™¯**ï¼š
- å¼€å‘è°ƒè¯•
- äººå·¥æŸ¥çœ‹æ—¥å¿—
- æ›´æ˜“è¯»çš„æ ¼å¼

### 2. å†…å­˜ç»Ÿè®¡æ’ä»¶ (memory)

å°†ç»Ÿè®¡æ•°æ®ä¿å­˜åœ¨å†…å­˜ä¸­ï¼Œé€‚åˆå¼€å‘å’Œæµ‹è¯•ã€‚

#### é…ç½®

```yaml
plugins:
  stats: ["memory"]

stats:
  memory:
    max_records: 1000  # æœ€å¤šä¿å­˜çš„è®°å½•æ•°
```

#### åŠŸèƒ½ç‰¹ç‚¹

- âœ… å®æ—¶ç»Ÿè®¡æ¯ä¸ªæ¨¡å‹çš„ä½¿ç”¨æƒ…å†µ
- âœ… è®°å½•æœ€è¿‘çš„è¯·æ±‚è¯¦æƒ…
- âœ… è‡ªåŠ¨è®¡ç®—å¹³å‡å€¼
- âœ… æœåŠ¡å…³é—­æ—¶è¾“å‡ºæ±‡æ€»ç»Ÿè®¡

#### ç»Ÿè®¡è¾“å‡ºç¤ºä¾‹

æœåŠ¡å…³é—­æˆ–é‡å¯æ—¶ä¼šè¾“å‡ºï¼š

```
============================================================
ğŸ“Š å†…å­˜ç»Ÿè®¡æ’ä»¶ - æœ€ç»ˆç»Ÿè®¡æ•°æ®
============================================================
æ€»è¯·æ±‚æ•°: 150

æ¨¡å‹: gpt-3.5-turbo
  â”œâ”€ æ€»è¯·æ±‚æ•°: 120
  â”œâ”€ è¾“å…¥ Token (prompt): 6,500
  â”œâ”€ è¾“å‡º Token (completion): 18,000
  â”œâ”€ æ€» Token: 24,500
  â”œâ”€ æ€»è€—æ—¶: 180.50s
  â”œâ”€ å¹³å‡ Token/è¯·æ±‚: 204.2
  â””â”€ å¹³å‡è€—æ—¶/è¯·æ±‚: 1.50s

æ¨¡å‹: gpt-4
  â”œâ”€ æ€»è¯·æ±‚æ•°: 30
  â”œâ”€ è¾“å…¥ Token (prompt): 2,100
  â”œâ”€ è¾“å‡º Token (completion): 5,500
  â”œâ”€ æ€» Token: 7,600
  â”œâ”€ æ€»è€—æ—¶: 75.30s
  â”œâ”€ å¹³å‡ Token/è¯·æ±‚: 253.3
  â””â”€ å¹³å‡è€—æ—¶/è¯·æ±‚: 2.51s

============================================================
```

### 3. åŒæ—¶ä½¿ç”¨å¤šä¸ªæ’ä»¶

å¯ä»¥åŒæ—¶å¯ç”¨å¤šä¸ªç»Ÿè®¡æ’ä»¶ï¼š

```yaml
plugins:
  stats: ["log", "memory"]

stats:
  log:
    format: "json"
  
  memory:
    max_records: 1000
```

## ğŸ“ˆ ç»Ÿè®¡æ•°æ®è¯´æ˜

### Token è®¡æ•°

#### è¾“å…¥ Token (prompt_tokens)
- åŒ…å«ç³»ç»Ÿæ¶ˆæ¯ã€ç”¨æˆ·æ¶ˆæ¯ã€å†å²å¯¹è¯ç­‰
- åœ¨å‘é€è¯·æ±‚å‰å°±å·²ç¡®å®š
- å½±å“è¯·æ±‚æˆæœ¬å’Œå“åº”æ—¶é—´

#### è¾“å‡º Token (completion_tokens)
- æ¨¡å‹ç”Ÿæˆçš„å“åº”æ–‡æœ¬
- åœ¨å“åº”å®Œæˆåæ‰èƒ½ç¡®å®š
- ä¸»è¦çš„è®¡è´¹é¡¹

#### æ€» Token (total_tokens)
```
total_tokens = prompt_tokens + completion_tokens
```

### æµå¼ vs éæµå¼

#### éæµå¼è¯·æ±‚
- Token ä¿¡æ¯ç›´æ¥ä»å“åº”çš„ `usage` å­—æ®µè·å–
- ç²¾ç¡®ä¸”å®æ—¶

#### æµå¼è¯·æ±‚
- Token ä¿¡æ¯ä»æœ€åçš„æ•°æ®å—æå–
- æˆ–é€šè¿‡ tiktoken ä¼°ç®—
- å¯èƒ½ç•¥æœ‰è¯¯å·®

## ğŸ”§ è‡ªå®šä¹‰ç»Ÿè®¡æ’ä»¶

æ‚¨å¯ä»¥å¼€å‘è‡ªå·±çš„ç»Ÿè®¡æ’ä»¶æ¥æ»¡è¶³ç‰¹å®šéœ€æ±‚ã€‚

### åˆ›å»ºè‡ªå®šä¹‰æ’ä»¶

```python
# my_stats_plugin/database_stats.py

from llm_one_api.plugins.interfaces import StatsPlugin
import psycopg2

class DatabaseStatsPlugin(StatsPlugin):
    """å°†ç»Ÿè®¡æ•°æ®å­˜å‚¨åˆ°æ•°æ®åº“"""
    
    def __init__(self, config):
        super().__init__(config)
        self.conn = psycopg2.connect(
            config.get("database_url")
        )
    
    async def record_response(self, response_info):
        """è®°å½•åˆ°æ•°æ®åº“"""
        cursor = self.conn.cursor()
        cursor.execute("""
            INSERT INTO api_stats (
                model, user_id, 
                prompt_tokens, completion_tokens, total_tokens,
                duration, timestamp
            ) VALUES (%s, %s, %s, %s, %s, %s, %s)
        """, (
            response_info['model'],
            response_info['user'],
            response_info['token_usage']['prompt_tokens'],
            response_info['token_usage']['completion_tokens'],
            response_info['token_usage']['total_tokens'],
            response_info['duration'],
            response_info['timestamp']
        ))
        self.conn.commit()
```

### æ³¨å†Œæ’ä»¶

```python
# setup.py
setup(
    entry_points={
        'llm_one_api.stats': [
            'database = my_stats_plugin.database_stats:DatabaseStatsPlugin',
        ],
    }
)
```

### ä½¿ç”¨æ’ä»¶

```yaml
plugins:
  stats: ["database"]

stats:
  database:
    database_url: "postgresql://user:pass@localhost/stats"
```

## ğŸ“Š ç»Ÿè®¡åˆ†æå»ºè®®

### 1. Token æˆæœ¬åˆ†æ

æ ¹æ®ç»Ÿè®¡æ•°æ®è®¡ç®—æˆæœ¬ï¼š

```python
# ä»·æ ¼ï¼ˆç¤ºä¾‹ï¼Œå•ä½ï¼šç¾å…ƒ/1K tokensï¼‰
PRICING = {
    "gpt-3.5-turbo": {
        "prompt": 0.0015,
        "completion": 0.002
    },
    "gpt-4": {
        "prompt": 0.03,
        "completion": 0.06
    }
}

def calculate_cost(model, prompt_tokens, completion_tokens):
    price = PRICING.get(model, PRICING["gpt-3.5-turbo"])
    
    prompt_cost = (prompt_tokens / 1000) * price["prompt"]
    completion_cost = (completion_tokens / 1000) * price["completion"]
    
    return prompt_cost + completion_cost

# ä¾‹å¦‚ï¼š
# gpt-3.5-turbo: 50 prompt + 120 completion = $0.00031
# gpt-4: 50 prompt + 120 completion = $0.0087
```

### 2. æ€§èƒ½ç›‘æ§æŒ‡æ ‡

å…³æ³¨ä»¥ä¸‹æŒ‡æ ‡ï¼š

- **å¹³å‡å“åº”æ—¶é—´** - åº”è¯¥åœ¨åˆç†èŒƒå›´å†…ï¼ˆ< 3ç§’ï¼‰
- **Token ä½¿ç”¨ç‡** - è¾“å…¥/è¾“å‡ºæ¯”ä¾‹æ˜¯å¦åˆç†
- **é”™è¯¯ç‡** - å¤±è´¥è¯·æ±‚å æ¯”
- **å¹¶å‘é‡** - æ´»è·ƒè¿æ¥æ•°

### 3. ç”¨æˆ·è¡Œä¸ºåˆ†æ

- å“ªäº›ç”¨æˆ·ä½¿ç”¨æœ€é¢‘ç¹ï¼Ÿ
- å“ªäº›æ¨¡å‹æœ€å—æ¬¢è¿ï¼Ÿ
- æµå¼ vs éæµå¼çš„ä½¿ç”¨æ¯”ä¾‹ï¼Ÿ
- é«˜å³°æ—¶æ®µåˆ†å¸ƒï¼Ÿ

## ğŸ¯ æœ€ä½³å®è·µ

### 1. æ—¥å¿—è½®è½¬

å¦‚æœä½¿ç”¨æ—¥å¿—ç»Ÿè®¡æ’ä»¶ï¼Œå»ºè®®é…ç½®æ—¥å¿—è½®è½¬ï¼š

```yaml
logging:
  file: "logs/stats.log"
  rotate: "daily"  # æˆ– "size:100MB"
  backup_count: 30
```

### 2. å®šæœŸå¯¼å‡º

å®šæœŸå°†å†…å­˜ç»Ÿè®¡æ•°æ®å¯¼å‡ºåˆ°æŒä¹…åŒ–å­˜å‚¨ï¼š

```python
# å®šæ—¶ä»»åŠ¡
async def export_stats():
    stats = memory_stats_plugin.get_stats()
    
    # å¯¼å‡ºåˆ°æ–‡ä»¶
    with open(f"stats_{datetime.now().strftime('%Y%m%d')}.json", 'w') as f:
        json.dump(stats, f, indent=2)
```

### 3. ç›‘æ§å‘Šè­¦

åŸºäºç»Ÿè®¡æ•°æ®è®¾ç½®å‘Šè­¦ï¼š

```python
# æ£€æŸ¥å¼‚å¸¸
if avg_tokens > 1000:
    alert("Token ä½¿ç”¨é‡è¿‡é«˜")

if error_rate > 0.05:
    alert("é”™è¯¯ç‡è¶…è¿‡ 5%")

if avg_duration > 5.0:
    alert("å“åº”æ—¶é—´è¿‡é•¿")
```

### 4. æˆæœ¬æ§åˆ¶

è®¾ç½®æ¯æ—¥/æ¯æœˆçš„ Token é™é¢ï¼š

```python
# æ¯æ—¥é™é¢æ£€æŸ¥
daily_tokens = sum(stats['total_tokens'] for stats in today_stats)

if daily_tokens > DAILY_LIMIT:
    # é™æµæˆ–åœæ­¢æœåŠ¡
    enable_rate_limiting()
```

## ğŸ“± å®æ—¶ç›‘æ§

ç»“åˆç›‘æ§å·¥å…·ä½¿ç”¨ï¼š

### Prometheus å¯¼å‡ºï¼ˆè®¡åˆ’ä¸­ï¼‰

```yaml
# æœªæ¥åŠŸèƒ½
monitoring:
  prometheus:
    enabled: true
    port: 9090
    metrics:
      - request_total
      - request_duration_seconds
      - token_usage_total
      - error_rate
```

### Grafana ä»ªè¡¨æ¿ï¼ˆè®¡åˆ’ä¸­ï¼‰

é¢„é…ç½®çš„ä»ªè¡¨æ¿æ˜¾ç¤ºï¼š
- å®æ—¶ QPS
- Token ä½¿ç”¨è¶‹åŠ¿
- å“åº”æ—¶é—´åˆ†å¸ƒ
- æ¨¡å‹ä½¿ç”¨å æ¯”
- æˆæœ¬è¶‹åŠ¿

## ğŸ” è°ƒè¯•å’Œæ’æŸ¥

### æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯

```bash
# å®æ—¶æŸ¥çœ‹æ—¥å¿—
tail -f logs/llm-one-api.log | grep "ğŸ“Š å“åº”ç»Ÿè®¡"

# JSON æ ¼å¼è§£æ
tail -f logs/llm-one-api.log | jq 'select(.event=="response")'

# ç»Ÿè®¡æ€» Token
tail -f logs/llm-one-api.log | jq 'select(.event=="response") | .tokens.total_tokens' | awk '{sum+=$1} END {print sum}'
```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [é…ç½®è¯´æ˜](../README.md#é…ç½®)
- [æ’ä»¶å¼€å‘](../examples/custom_plugin/README.md)
- [API å‚è€ƒ](../README.md#api-æ–‡æ¡£)

## ğŸ¤ è´¡çŒ®

å¦‚æœæ‚¨å¼€å‘äº†æœ‰ç”¨çš„ç»Ÿè®¡æ’ä»¶ï¼Œæ¬¢è¿æäº¤ PR åˆ†äº«ç»™ç¤¾åŒºï¼

